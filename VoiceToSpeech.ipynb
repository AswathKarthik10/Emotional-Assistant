{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git \n",
    "!pip install -q pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q io wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q edge-tts\n",
    "!pip install -q asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import edge_tts\n",
    "import webrtcvad\n",
    "import wave\n",
    "import io\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "import pyaudio\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genai is configured with the API key.\n"
     ]
    }
   ],
   "source": [
    "# Read the API key from the config.json file\n",
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "    api_key = config['api_key']\n",
    "\n",
    "# Configure genai with the API key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Now you can use genai functions as needed\n",
    "print(\"genai is configured with the API key.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = genai.upload_file(\"output_test.wav\")\n",
    "print(f\"{myfile=}\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "result = model.generate_content([myfile, \"You are an emotional assistant. You help people by listening to them and provide them with appropriate advice, but mainly you listen to them and lend them an ear. Give a suitable response based on the audio and maintain a conversational style. The response can be a follow up question, or a piece of advice, or anything that you deem as suitable as an emotional assistant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = result.text\n",
    "VOICE = \"en-US-JennyNeural\"\n",
    "OUTPUT_FILE = \"test3.mp3\"\n",
    "WEBVTT_FILE = \"test3.vtt\"\n",
    "\n",
    "\n",
    "async def amain() -> None:\n",
    "    \"\"\"Main function\"\"\"\n",
    "    communicate = edge_tts.Communicate(TEXT, VOICE)\n",
    "    submaker = edge_tts.SubMaker()\n",
    "    with open(OUTPUT_FILE, \"wb\") as file:\n",
    "        async for chunk in communicate.stream():\n",
    "            if chunk[\"type\"] == \"audio\":\n",
    "                file.write(chunk[\"data\"])\n",
    "            elif chunk[\"type\"] == \"WordBoundary\":\n",
    "                submaker.create_sub((chunk[\"offset\"], chunk[\"duration\"]), chunk[\"text\"])\n",
    "\n",
    "    with open(WEBVTT_FILE, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(submaker.generate_subs())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await(amain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import webrtcvad\n",
    "import collections\n",
    "import time\n",
    "import sys\n",
    "import wave\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "# Audio recording parameters\n",
    "RATE = 16000\n",
    "CHANNELS = 1\n",
    "FORMAT = pyaudio.paInt16\n",
    "FRAME_DURATION = 30  # ms\n",
    "FRAME_SIZE = int(RATE * FRAME_DURATION / 1000)  # Number of audio samples per frame\n",
    "\n",
    "# Load configuration from config.json\n",
    "def load_config():\n",
    "    with open('config.json', 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    return config\n",
    "\n",
    "# Setup Google Cloud credentials from config.json\n",
    "config = load_config()\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config['api_key']\n",
    "\n",
    "# Voice Activity Detector\n",
    "vad = webrtcvad.Vad(2)  # Level 2 sensitivity\n",
    "\n",
    "# Google Cloud Speech client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "def record_audio():\n",
    "    \"\"\"Records audio from the microphone in real-time.\"\"\"\n",
    "    audio_interface = pyaudio.PyAudio()\n",
    "\n",
    "    # Set up audio stream\n",
    "    stream = audio_interface.open(format=FORMAT,\n",
    "                                  channels=CHANNELS,\n",
    "                                  rate=RATE,\n",
    "                                  input=True,\n",
    "                                  frames_per_buffer=FRAME_SIZE)\n",
    "\n",
    "    print(\"Recording... Press Ctrl+C to stop.\")\n",
    "    frames = collections.deque(maxlen=int(10 * 1000 / FRAME_DURATION))  # Store last 10 seconds of audio\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frame = stream.read(FRAME_SIZE, exception_on_overflow=False)\n",
    "\n",
    "            # Use VAD to check if the frame contains speech\n",
    "            is_speech = vad.is_speech(frame, RATE)\n",
    "\n",
    "            if is_speech:\n",
    "                frames.append(frame)\n",
    "            elif len(frames) > 0:\n",
    "                # Process and transcribe audio when speech ends\n",
    "                audio_data = b''.join(frames)\n",
    "                transcribe_audio(audio_data)\n",
    "                frames.clear()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping...\")\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio_interface.terminate()\n",
    "\n",
    "def transcribe_audio(audio_data):\n",
    "    \"\"\"Transcribes the given audio data using Google Speech-to-Text.\"\"\"\n",
    "    audio = types.RecognitionAudio(content=audio_data)\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=\"en-US\"\n",
    "    )\n",
    "\n",
    "    # Perform the transcription request\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "    # Print the transcription result\n",
    "    for result in response.results:\n",
    "        print(\"Transcription: {}\".format(result.alternatives[0].transcript))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Audio recording parameters\n",
    "RATE = 16000  # Sample rate\n",
    "CHANNELS = 1  # Number of input channels (mono)\n",
    "FORMAT = pyaudio.paInt16  # 16-bit audio\n",
    "CHUNK = 1024  # Buffer size\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio_interface = pyaudio.PyAudio()\n",
    "\n",
    "# Open the microphone stream\n",
    "stream = audio_interface.open(\n",
    "    format=FORMAT,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    frames_per_buffer=CHUNK\n",
    ")\n",
    "\n",
    "print(\"Recording... Press Ctrl+C to stop.\")\n",
    "\n",
    "# Helper function to convert audio buffer to numpy array\n",
    "def buffer_to_numpy(audio_data):\n",
    "    return np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "try:\n",
    "    # Recording and transcription loop\n",
    "    while True:\n",
    "        # Read audio data from the stream\n",
    "        audio_data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        \n",
    "        # Convert audio data to numpy array and perform transcription\n",
    "        audio_array = buffer_to_numpy(audio_data)\n",
    "        \n",
    "        # Use Whisper's transcribe function on the audio chunk\n",
    "        # To make transcription real-time, we use shorter audio chunks\n",
    "        result = model.transcribe(audio_array, fp16=torch.cuda.is_available(), language=\"en\", task=\"transcribe\")\n",
    "        \n",
    "        # Print the transcription\n",
    "        if \"text\" in result:\n",
    "            print(\"Transcription:\", result[\"text\"])\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Cleanup when user interrupts the recording\n",
    "    print(\"Stopping...\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playsound import playsound\n",
    "\n",
    "audio_file = \"output_test.wav\"\n",
    "playsound(audio_file)\n",
    "\n",
    "print(\"Playback finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q simpleaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start recording...\n",
      "Silence detected. Stopping recording.\n",
      "myfile=genai.File({\n",
      "    'name': 'files/2j0nbtux8sp1',\n",
      "    'display_name': 'output_test.wav',\n",
      "    'mime_type': 'audio/wav',\n",
      "    'sha256_hash': 'NDllMTZmNGYwNDBhOTc1NjE5YTJiMzVmNGI3MjNmMDMzMGFjNWU1ZjNkOGE3OTljNjM5NzEyODlmM2Q5OWU2MQ==',\n",
      "    'size_bytes': '1369644',\n",
      "    'state': 'ACTIVE',\n",
      "    'uri': 'https://generativelanguage.googleapis.com/v1beta/files/2j0nbtux8sp1',\n",
      "    'create_time': '2024-10-19T14:16:22.068774Z',\n",
      "    'expiration_time': '2024-10-21T14:16:21.994544844Z',\n",
      "    'update_time': '2024-10-19T14:16:22.068774Z'})\n"
     ]
    }
   ],
   "source": [
    "#Input speech from user\n",
    "FRAMES_PER_BUFFER = 3200\n",
    "CHANNELS = 1\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "SILENCE_THRESHOLD = 100  # Amplitude threshold for silence detection\n",
    "SILENCE_DURATION = 3  # Duration in seconds to consider as silence\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(\n",
    "    format=FORMAT,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    frames_per_buffer=FRAMES_PER_BUFFER\n",
    ")\n",
    "\n",
    "print('start recording...')\n",
    "\n",
    "frames = []\n",
    "silent_chunks = 0\n",
    "silence_limit = int(RATE / FRAMES_PER_BUFFER * SILENCE_DURATION)\n",
    "\n",
    "while True:\n",
    "    data = stream.read(FRAMES_PER_BUFFER)\n",
    "    frames.append(data)\n",
    "\n",
    "    # Convert audio data to numpy array to analyze sound level\n",
    "    audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "    amplitude = np.abs(audio_data).mean()\n",
    "\n",
    "    # Check if the amplitude is below the silence threshold\n",
    "    if amplitude < SILENCE_THRESHOLD:\n",
    "        silent_chunks += 1\n",
    "    else:\n",
    "        silent_chunks = 0\n",
    "\n",
    "    # If silence has lasted for the specified duration, stop recording\n",
    "    if silent_chunks > silence_limit:\n",
    "        print(\"Silence detected. Stopping recording.\")\n",
    "        break\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "# Save the recorded audio to a file\n",
    "output_file = \"output_test.wav\"\n",
    "with wave.open(output_file, 'wb') as obj:\n",
    "    obj.setnchannels(CHANNELS)\n",
    "    obj.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    obj.setframerate(RATE)\n",
    "    obj.writeframes(b\"\".join(frames))\n",
    "\n",
    "#Process the message using the model\n",
    "myfile = genai.upload_file(\"output_test.wav\")\n",
    "print(f\"{myfile=}\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "prompt = \"You are an emotional assistant. You help people by listening to them and provide them with appropriate advice, but mainly you listen to them and lend them an ear. Give a suitable response based on the audio and maintain a conversational style. Acknowledge the positive things mentioned by them. Furthermore, the response can be a follow up question, or a piece of advice, or anything that you deem as suitable as an emotional assistant\"\n",
    "result = model.generate_content([myfile, ])\n",
    "\n",
    "#Perform TTS conversion to play the output audio\n",
    "TEXT = result.text\n",
    "VOICE = \"en-US-JennyNeural\"\n",
    "OUTPUT_FILE = \"test3.mp3\"\n",
    "WEBVTT_FILE = \"test3.vtt\"\n",
    "\n",
    "\n",
    "async def amain() -> None:\n",
    "    \"\"\"Main function\"\"\"\n",
    "    communicate = edge_tts.Communicate(TEXT, VOICE)\n",
    "    submaker = edge_tts.SubMaker()\n",
    "    with open(OUTPUT_FILE, \"wb\") as file:\n",
    "        async for chunk in communicate.stream():\n",
    "            if chunk[\"type\"] == \"audio\":\n",
    "                file.write(chunk[\"data\"])\n",
    "            elif chunk[\"type\"] == \"WordBoundary\":\n",
    "                submaker.create_sub((chunk[\"offset\"], chunk[\"duration\"]), chunk[\"text\"])\n",
    "\n",
    "    with open(WEBVTT_FILE, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(submaker.generate_subs())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await(amain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start recording...\n",
      "Silence detected. Stopping recording.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 622, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2014, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1567, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 846, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aswat\\AppData\\Local\\Temp\\ipykernel_27600\\3340455065.py\", line 158, in start_recording\n",
      "    response, audio_file_response, red_flag_detected, red_flag_message = asyncio.run(talk())\n",
      "                                                                         ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\runners.py\", line 194, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aswat\\AppData\\Local\\Temp\\ipykernel_27600\\3340455065.py\", line 146, in talk\n",
      "    response, audio_file_response, red_flag_detected, red_flag_message = await generate_response(audio_file)  # Generate response\n",
      "                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aswat\\AppData\\Local\\Temp\\ipykernel_27600\\3340455065.py\", line 91, in generate_response\n",
      "    transcription = await transcribe_audio(audio_file)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aswat\\AppData\\Local\\Temp\\ipykernel_27600\\3340455065.py\", line 80, in transcribe_audio\n",
      "    myfile = genai.upload_file(audio_file)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\google\\generativeai\\files.py\", line 85, in upload_file\n",
      "    response = client.create_file(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\google\\generativeai\\client.py\", line 121, in create_file\n",
      "    result = request.execute()\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\_helpers.py\", line 130, in positional_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py\", line 902, in execute\n",
      "    _, body = self.next_chunk(http=http, num_retries=num_retries)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\_helpers.py\", line 130, in positional_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py\", line 1007, in next_chunk\n",
      "    resp, content = _retry_request(\n",
      "                    ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py\", line 222, in _retry_request\n",
      "    raise exception\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py\", line 191, in _retry_request\n",
      "    resp, content = http.request(uri, method, *args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py\", line 1724, in request\n",
      "    (response, content) = self._request(\n",
      "                          ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py\", line 1444, in _request\n",
      "    (response, content) = self._conn_request(conn, request_uri, method, body, headers)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py\", line 1367, in _conn_request\n",
      "    conn.request(method, request_uri, body, headers)\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1336, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1382, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1331, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1091, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1055, in send\n",
      "    self.sock.sendall(data)\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1210, in sendall\n",
      "    v = self.send(byte_view[count:])\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1179, in send\n",
      "    return self._sslobj.write(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:2406)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start recording...\n",
      "Silence detected. Stopping recording.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py\", line 1396, in _conn_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 300, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 622, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2014, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1567, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 846, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aswat\\AppData\\Local\\Temp\\ipykernel_27600\\3340455065.py\", line 158, in start_recording\n",
      "    response, audio_file_response, red_flag_detected, red_flag_message = asyncio.run(talk())\n",
      "                                                                         ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\runners.py\", line 194, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aswat\\AppData\\Local\\Temp\\ipykernel_27600\\3340455065.py\", line 146, in talk\n",
      "    response, audio_file_response, red_flag_detected, red_flag_message = await generate_response(audio_file)  # Generate response\n",
      "                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aswat\\AppData\\Local\\Temp\\ipykernel_27600\\3340455065.py\", line 91, in generate_response\n",
      "    transcription = await transcribe_audio(audio_file)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aswat\\AppData\\Local\\Temp\\ipykernel_27600\\3340455065.py\", line 80, in transcribe_audio\n",
      "    myfile = genai.upload_file(audio_file)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\google\\generativeai\\files.py\", line 85, in upload_file\n",
      "    response = client.create_file(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\google\\generativeai\\client.py\", line 121, in create_file\n",
      "    result = request.execute()\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\_helpers.py\", line 130, in positional_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py\", line 902, in execute\n",
      "    _, body = self.next_chunk(http=http, num_retries=num_retries)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\_helpers.py\", line 130, in positional_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py\", line 1007, in next_chunk\n",
      "    resp, content = _retry_request(\n",
      "                    ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py\", line 222, in _retry_request\n",
      "    raise exception\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py\", line 191, in _retry_request\n",
      "    resp, content = http.request(uri, method, *args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py\", line 1724, in request\n",
      "    (response, content) = self._request(\n",
      "                          ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py\", line 1444, in _request\n",
      "    (response, content) = self._conn_request(conn, request_uri, method, body, headers)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py\", line 1405, in _conn_request\n",
      "    conn.connect()\n",
      "  File \"c:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py\", line 1156, in connect\n",
      "    sock.connect((self.host, self.port))\n",
      "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "import pyaudio\n",
    "import gradio as gr\n",
    "import edge_tts\n",
    "import asyncio\n",
    "from textblob import TextBlob  # Import for sentiment analysis\n",
    "\n",
    "# Audio settings\n",
    "FRAMES_PER_BUFFER = 3200\n",
    "CHANNELS = 1\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "SILENCE_THRESHOLD = 100  # Amplitude threshold for silence detection\n",
    "SILENCE_DURATION = 3  # Duration in seconds to consider as silence\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Initialize conversation context\n",
    "conversation_history = []\n",
    "\n",
    "# List of phrases that could indicate red flags\n",
    "RED_FLAG_KEYWORDS = [\n",
    "    \"bad\", \"hate\", \"angry\", \"upset\", \"frustrated\", \"stress\", \"worried\",\n",
    "    \"sick\", \"negative\", \"problem\", \"sad\", \"feel bad\", \"don't want\",\n",
    "    \"no\", \"never\", \"can't\", \"avoid\", \"give up\", \"hopeless\"\n",
    "]\n",
    "\n",
    "async def process_audio():\n",
    "    # Start recording audio\n",
    "    stream = p.open(\n",
    "        format=FORMAT,\n",
    "        channels=CHANNELS,\n",
    "        rate=RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=FRAMES_PER_BUFFER\n",
    "    )\n",
    "\n",
    "    print('start recording...')\n",
    "    \n",
    "    frames = []\n",
    "    silent_chunks = 0\n",
    "    silence_limit = int(RATE / FRAMES_PER_BUFFER * SILENCE_DURATION)\n",
    "\n",
    "    while True:\n",
    "        data = stream.read(FRAMES_PER_BUFFER)\n",
    "        frames.append(data)\n",
    "\n",
    "        # Convert audio data to numpy array to analyze sound level\n",
    "        audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "        amplitude = np.abs(audio_data).mean()\n",
    "\n",
    "        # Check if the amplitude is below the silence threshold\n",
    "        if amplitude < SILENCE_THRESHOLD:\n",
    "            silent_chunks += 1\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "\n",
    "        # If silence has lasted for the specified duration, stop recording\n",
    "        if silent_chunks > silence_limit:\n",
    "            print(\"Silence detected. Stopping recording.\")\n",
    "            break\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    # Save the recorded audio to a file\n",
    "    output_file = \"output_test.wav\"\n",
    "    with wave.open(output_file, 'wb') as obj:\n",
    "        obj.setnchannels(CHANNELS)\n",
    "        obj.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        obj.setframerate(RATE)\n",
    "        obj.writeframes(b\"\".join(frames))\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "async def transcribe_audio(audio_file):\n",
    "    \"\"\"Transcribe the audio file to text using the generative model.\"\"\"\n",
    "    myfile = genai.upload_file(audio_file)\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    result = model.generate_content([myfile, \"Transcribe what you hear in the audio file\"])\n",
    "    response_text = result.text\n",
    "    return response_text\n",
    "\n",
    "async def generate_response(audio_file):\n",
    "    global conversation_history\n",
    "\n",
    "    # Transcribe the audio file to text\n",
    "    transcription = await transcribe_audio(audio_file)\n",
    "\n",
    "    # Detect red flags in the transcription\n",
    "    red_flag_detected, red_flag_message = detect_red_flags(transcription)\n",
    "\n",
    "    # Process the audio file with your model\n",
    "    myfile = genai.upload_file(audio_file)\n",
    "    print(f\"{myfile=}\")\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    # Build the context by combining previous messages\n",
    "    context = \"\\n\".join(conversation_history[-5:])  # Get the last 5 messages\n",
    "    prompt = (f\"You are an emotional assistant. You help people by listening to them and provide them \"\n",
    "              \"with appropriate advice, but mainly you listen to them and lend them an ear. \"\n",
    "              f\"Previous messages:\\n{context}\\n\"\n",
    "              \"Give a suitable response based on the audio and maintain a conversational style. \"\n",
    "              \"Acknowledge the positive things mentioned by them. \"\n",
    "              \"Furthermore, the response can be a follow-up question, or a piece of advice, or anything \"\n",
    "              \"that you deem as suitable as an emotional assistant.\")\n",
    "\n",
    "    result = model.generate_content([myfile, prompt])\n",
    "    response_text = result.text\n",
    "    conversation_history.append(response_text)  # Add response to history\n",
    "\n",
    "    # Convert text response to speech and save as audio file\n",
    "    response_audio_file = \"response_audio.mp3\"\n",
    "    await text_to_speech(response_text, response_audio_file)\n",
    "\n",
    "    return response_text, response_audio_file, red_flag_detected, red_flag_message\n",
    "\n",
    "def detect_red_flags(transcription):\n",
    "    \"\"\"Detect red flags in the transcription based on sentiment analysis and keywords.\"\"\"\n",
    "    # Check for red flag keywords\n",
    "    for keyword in RED_FLAG_KEYWORDS:\n",
    "        if keyword in transcription.lower():\n",
    "            return True, \"Detected negative sentiment related to: \" + keyword\n",
    "\n",
    "    # Analyze sentiment\n",
    "    sentiment = TextBlob(transcription).sentiment\n",
    "    if sentiment.polarity < 0:  # Negative sentiment\n",
    "        return True, \"Detected negative sentiment.\"\n",
    "\n",
    "    return False, \"\"\n",
    "\n",
    "async def text_to_speech(text, output_file):\n",
    "    \"\"\"Convert text to speech using edge-tts and save as audio file.\"\"\"\n",
    "    communicate = edge_tts.Communicate(text, \"en-US-JennyNeural\")\n",
    "    with open(output_file, \"wb\") as file:\n",
    "        async for chunk in communicate.stream():\n",
    "            if chunk[\"type\"] == \"audio\":\n",
    "                file.write(chunk[\"data\"])\n",
    "\n",
    "async def talk():\n",
    "    audio_file = await process_audio()  # Record audio\n",
    "    response, audio_file_response, red_flag_detected, red_flag_message = await generate_response(audio_file)  # Generate response\n",
    "    return response, audio_file_response, red_flag_detected, red_flag_message\n",
    "\n",
    "def gradio_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"### Emotional Assistant\")\n",
    "        audio_button = gr.Button(\"Start Recording\")\n",
    "        output_text = gr.Textbox(label=\"Assistant Response\", interactive=False)\n",
    "        audio_output = gr.Audio(label=\"Response Audio\", type=\"filepath\")\n",
    "        red_flag_output = gr.Textbox(label=\"Red Flag Detection\", interactive=False)\n",
    "\n",
    "        def start_recording():\n",
    "            response, audio_file_response, red_flag_detected, red_flag_message = asyncio.run(talk())\n",
    "            if red_flag_detected:\n",
    "                red_flag_message = f\"⚠️ Red Flag Detected: {red_flag_message}\"\n",
    "            else:\n",
    "                red_flag_message = \"No red flags detected.\"\n",
    "            return response, audio_file_response, red_flag_message\n",
    "\n",
    "        audio_button.click(fn=start_recording, outputs=[output_text, audio_output, red_flag_output])\n",
    "\n",
    "    demo.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gradio_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file):\n",
    "    myfile = genai.upload_file(audio_file)\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    result = model.generate_content([myfile, \"Transcribe what you hear in the audio file\"])\n",
    "    response_text = result.text\n",
    "    print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_test.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[1;34m(audio_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe_audio\u001b[39m(audio_file):\n\u001b[1;32m----> 2\u001b[0m     myfile \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_test.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content([myfile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscribe what you hear in the audio file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\google\\generativeai\\files.py:85\u001b[0m, in \u001b[0;36mupload_file\u001b[1;34m(path, mime_type, name, display_name, resumable)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[0;32m     83\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 85\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmime_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresumable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresumable\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_types\u001b[38;5;241m.\u001b[39mFile(response)\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\google\\generativeai\\client.py:101\u001b[0m, in \u001b[0;36mFileServiceClient.create_file\u001b[1;34m(self, path, mime_type, name, display_name, resumable, metadata)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_file\u001b[39m(\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     92\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike \u001b[38;5;241m|\u001b[39m IOBase,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m     metadata: Sequence[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m (),\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m protos\u001b[38;5;241m.\u001b[39mFile:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discovery_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_discovery_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     file \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\google\\generativeai\\client.py:82\u001b[0m, in \u001b[0;36mFileServiceClient._setup_discovery_api\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid operation: Uploading to the File API requires an API key. Please provide a valid API key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m     )\n\u001b[0;32m     76\u001b[0m request \u001b[38;5;241m=\u001b[39m googleapiclient\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mHttpRequest(\n\u001b[0;32m     77\u001b[0m     http\u001b[38;5;241m=\u001b[39mhttplib2\u001b[38;5;241m.\u001b[39mHttp(),\n\u001b[0;32m     78\u001b[0m     postproc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m resp, content: (resp, content),\n\u001b[0;32m     79\u001b[0m     uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGENAI_API_DISCOVERY_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?version=v1beta&key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     80\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(metadata),\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m response, content \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m request\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     85\u001b[0m discovery_doc \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py:923\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody))\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# Handle retries for server-side errors.\u001b[39;00m\n\u001b[1;32m--> 923\u001b[0m resp, content \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_callbacks:\n\u001b[0;32m    936\u001b[0m     callback(resp)\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py:222\u001b[0m, in \u001b[0;36m_retry_request\u001b[1;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry_num \u001b[38;5;241m==\u001b[39m num_retries:\n\u001b[1;32m--> 222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\googleapiclient\\http.py:191\u001b[0m, in \u001b[0;36m_retry_request\u001b[1;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     resp, content \u001b[38;5;241m=\u001b[39m \u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Retry on SSL errors and socket timeout errors.\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _ssl_SSLError \u001b[38;5;28;01mas\u001b[39;00m ssl_error:\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py:1724\u001b[0m, in \u001b[0;36mHttp.request\u001b[1;34m(self, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[0;32m   1722\u001b[0m             content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1723\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1724\u001b[0m             (response, content) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthority\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mredirections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcachekey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1728\u001b[0m     is_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(e, socket\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py:1444\u001b[0m, in \u001b[0;36mHttp._request\u001b[1;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth:\n\u001b[0;32m   1442\u001b[0m     auth\u001b[38;5;241m.\u001b[39mrequest(method, request_uri, headers, body)\n\u001b[1;32m-> 1444\u001b[0m (response, content) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth:\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m auth\u001b[38;5;241m.\u001b[39mresponse(response, body):\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py:1366\u001b[0m, in \u001b[0;36mHttp._conn_request\u001b[1;34m(self, conn, request_uri, method, body, headers)\u001b[0m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1366\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1367\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(method, request_uri, body, headers)\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout:\n",
      "File \u001b[1;32mc:\\Users\\aswat\\OneDrive\\Documents\\Voice\\.venv\\Lib\\site-packages\\httplib2\\__init__.py:1156\u001b[0m, in \u001b[0;36mHTTPSConnectionWithTimeout.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_timeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout):\n\u001b[0;32m   1155\u001b[0m     sock\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[1;32m-> 1156\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost)\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;66;03m# Python 3.3 compatibility: emulate the check_hostname behavior\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond"
     ]
    }
   ],
   "source": [
    "transcribe_audio('output_test.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
